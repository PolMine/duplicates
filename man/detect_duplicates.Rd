% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/detect_duplicates.R
\name{detect_duplicates}
\alias{detect_duplicates}
\alias{detect_duplicates,partition_bundle-method}
\alias{detect_duplicates,list-method}
\alias{detect_duplicates,dgCMatrix-method}
\title{Detect Duplicates}
\usage{
detect_duplicates(x, ...)

\S4method{detect_duplicates}{partition_bundle}(
  x,
  n = 5L,
  min_shingle_length = n,
  p_attribute = "word",
  s_attribute = "text_date",
  vocab,
  threshold = 0.9,
  verbose = TRUE,
  mc = FALSE
)

\S4method{detect_duplicates}{list}(
  x,
  n = 5L,
  min_shingle_length = n,
  char = "",
  threshold = 0.9,
  verbose = TRUE,
  mc = FALSE
)

\S4method{detect_duplicates}{dgCMatrix}(x, n, min_shingle_length, threshold, verbose)
}
\arguments{
\item{x}{A \code{partition_bundle} or \code{subcorpus_bundle} object with documents to
evaluate.}

\item{...}{Further arguments (unused).}

\item{n}{The number of characters to use for shingling (\code{integer} value),
passed as argument \code{n} into \code{polmineR::ngrams()}. Defaults to 5, in
line with Kliche et al. 2014: 695.}

\item{min_shingle_length}{An \code{integer} value with minimum length of
shingles that enter calculation of document similarity. Defaults to
\code{n}.}

\item{p_attribute}{The p-attribute to evaluate.}

\item{s_attribute}{The s-attribute providing the date of documents.}

\item{vocab}{Pruned vocabulary.}

\item{threshold}{A \code{numeric} value (0 < x < 1), the minimum similarity to
qualify two documents as duplicates}

\item{verbose}{A \code{logical} value, whether to be verbose.}

\item{mc}{A \code{logical} value, whether to use multicore.}

\item{char}{A \code{character} vector with characters to keep. Passed into method
\code{polmineR::ngrams()}.}
}
\value{
The updated content of slot \verb{$duplicates} is returned invisibly.
}
\description{
Class for duplicate detection.
}
\details{
The class implements a procedure described by Fritz Kliche, Andre Blessing,
Urlich Heid and Jonathan Sonntag in the paper "The eIdentity Text
ExplorationWorkbench" presented at LREC 2014
(see \url{http://www.lrec-conf.org/proceedings/lrec2014/pdf/332_Paper.pdf}).

To detect duplicates, choices are made as follows:
\itemize{
\item If two similar articles have been published on the same day, the shorter
article will be considered the duplicate;
\item if two similar articles were published on different days, the article that
appeared later will be considered the duplicate.
}
}
\examples{
library(polmineR)
use(pkg = "duplicates")

charcount <- corpus("REUTERS2") \%>\% 
  charcount(
    p_attribute = "word",
    char_regex = "[a-zA-Z]",
    lowercase = TRUE,
    decreasing = FALSE
   )

vocab <- corpus("REUTERS2") \%>\% 
  p_attributes(p_attribute = "word") \%>\% 
  charfilter(chars = names(charcount[1:12]))

x <- corpus("REUTERS2") |>
  split(s_attribute = "doc_id")

dupl <- detect_duplicates(
    x = x,
    p_attribute = "word",
    s_attribute = "doc_id",
    mc = parallel::detectCores() - 2L,
    vocab = vocab
  )
  
docgrps <- as_docgroups(dupl)
library(polmineR)
use(pkg = "duplicates")

x <- corpus("REUTERS2") \%>\% 
  split(s_attribute = "doc_id") \%>\% 
  get_token_stream(p_attribute = "word", collapse = "")
  
chars <- table(tolower(strsplit(paste(unlist(x), collapse = ""), "")[[1]]))
chars <- chars[grep("[a-zA-Z]", names(chars))]
char <- names(chars[order(chars, decreasing = FALSE)][1:20])

dupl <- detect_duplicates(x = x, n = 5L, char = char, threshold = 0.6)

docgrps <- as_docgroups(dupl, cols = "name", order = 1L)
}
